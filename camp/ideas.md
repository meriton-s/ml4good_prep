# to do:
.
## colab notebooks
- colab notebook on hyperparametrs -- visualisation (to demostrate dependansies)
## readins 
- Against Almost Every Theory of Impact of Interpretability by Charbel-Raphael Segerie
https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1
- Theory of change
https://forum.effectivealtruism.org/topics/theory-of-change

## video
- watch youtube about alphaEvolve
- https://arena-chapter3-llm-evals.streamlit.app/

https://docs.google.com/presentation/d/1zDlr4dxmmC642383OPRsm2a7tOhrA5oOPyndC5CDn64/edit?slide=id.g35f19ef10d6_34_6#slide=id.g35f19ef10d6_34_6

# to desing
## evals 
- try to create a funny evals like vending-bench but about ai safety management

## ai safety meme contests
- offer api keys to gpt, do weekly on start in ai safety

## colab notebook
- all the notebooks in versions: easy -- read and play normal: oneinstruction one line of code hard: explanation of rhe task + code structure scratch: instruction how to do it in user's ide)
- it would be cool to create a course there one builds NN step by step: classifier - hand -written number recognition - adverserial attacks - etc. It could kind of help keep the whole picture in one's mind
- NN fundamentals: from partial derivatives to groking:
  - MLP on numpy from scratch
  - torch 
  - hyperparamets 
  - groking & double decent 

## workshop on intro to AISR \ carrier
- Skills
  * the world will down-rate you on the lack of the following
    * networking
    * fundraising
    * public\research communication
    * technical skills (low level\grounded skills)
    * strategic\systemic views (high level understanding)
    * meta-skills (research taste, time management, self reflection and open-mindedness)
* concrete scenarios are bullshit (reading from preparation) but human mind are kinda designed to generate it. here is an attempt to use the free ability to generate plans and turn it into some estimations that makes sense. how to produce usefully scenario activities: 
  - imagine a concrete scenario
  - divide it into steps if not done before
  - start from a beginning and estimate how many equally dangerous alternatives you can name
  - repeat to each step, making a tree
  - estimate a resulting probability\likelihood or whatever you want
* you have 100m $ and 2 years -- build a strategy to solve ai safety problem
* you have a strategy -- make step by step plan to implement it
* 

## governance post
- risk classification based of what's broken not whom to blame
  - ## Risks classification
  - Balance disruption
  - Bad attractor risks
  - Singularity risks
  - Absense of AI risks
  
  - ## Solutions
  * Fix what exists
  
we do not fit our own standards for software with fixing tools -- it is not working properly.  
  * Build correct from the beginning

we do not have a winning approach, and we do not use best practises humanity has to generate approaches 
  * Stop doing (bigger) AI's

what progress do we have on it? ias it just talking as about the climate change?

- ## GD like dynamics 
  - what systems are most fragile? (complex? high bared?)
  - what systems are in unstable equilibrium
  - where do we transfer control
  - where are most decisions are made by ai already
  - where ai shapes human behaviour 


# to think 
## theory of change 
- why is methodology in ai safety is so messy?
- why do i think it is messy? - lack of efforts to exclude possibilities for x to happen/to work/ to bee true

## exercise on strategy 
- i'm mostly worried about X
- x mostly happens because of Y
- i can change Y by doing Z

## llms & space of tokens  
- LLMs are better in coding because the space of code is so much smaller than of natural level
- if that is true then it also would be true for analytic\synthetic languages? And artificial languages? And other structures with smaller space?
- it is interesting to think about all of this from the point of configuration space
- hy do people think that features exist? that embedding correspond to our concepts behind words? what minimum experiment could prove it wrong? 
  - we were shown that addition work for embeddings but what about our assumption of the meaning of each parameter in this sum? is it possible to show that same embeddings do not form transparent addition dynamic on addition? if king+women returns queen, does it also respect king + child? or better does kind minus child correspond to anything meaningful 

## incentives of arm race
competitive pressure might work not as i use to think "if we don't do bad thing those awful guys we are competing with will do even a worse thing". But as "we don't want to lose to those awful (or any) guys we are competing with"

## hybrid architectures
- compare 
  - voyager (Wang 2023) that use gpt generating instructions and code that implement them communicating via ApI (knowing the state of the game)
  - IGOR that use llm+rl
- llm+rl worked, llm+evoAlg worked. what else can be mixed with llm?

## best go player
- what exactly is going on with alfaGo\kataGo and adversarials? the issue is that winning in go is such a complex landscape that it doesn't generalize well. but why is it so? is it something to do with sampling? or is it a nature of the task?



# to google
- i should take a look on the papers of world models in LLM (golden bridge) + "on the biology of large language models" by Anthropic

- google about figure 01 + openAI: they are doing speech reasoning in a voice controlled robot
- theory of mind benchmarks and self\others overlap
- mirror test: YouTube, scholar, miriY

- what's the temperature of llm, how does it work
- is there a video on dimond maximizer&strawberry problem

# to skill up
### NN branch
- do NN from scratch
- do torch for NN
- do hyperparamets & grokking

### RL branch
- do openAIgym with gpt (collection of games with safety easter eggs)
- do rl on coursera
- do rlfh

### LLM branch
- do llm from scratch
- do bayesian inference to llm
- do slt theory 
- do temaeus exercises 

