# 12.05.2025 – ML4GOOD research log

## what i’m doing today
- play with pytorch by fitting a sin curve with a degree-3 polynomial
- rewrite manual gradient descent from numpy to torch
- read Soares and take notes

## stream of consciousness
dropped into PyCharm. i know colab is totally fine for the program itself, but right now i just want my familiar dark dark theme and full control. pre-filled cells make me feel like i’m in third grade again. seriously, it kills the magic of programming. i sound like a 100-year-old grumbling about punch cards. anyway.

started with the numpy version — things looked clean until i tried to write the gradients myself.  
after ~10 minutes of wrangling with partial derivatives, i got hooked. might turn this into a mini-tutorial notebook — deriving things from first principles is actually kind of fun.

then spent ~15 minutes yelling at `plt.legend()` — nothing worked. gave up. green line looks weird anyway.

also had a mild identity crisis over whether `mean()` and `E[...]` are the same thing. they are, right? just one's mathy and the other's in code. right?

read *Half-assing it with everything you’ve got*. seems less about effort vs laziness and more about not outsourcing your values to external standards. i don’t fully get it yet. maybe Scott Alexander’s graduation speech feels like it’s circling something similar. need to reread tomorrow.

## results
- working pytorch version of cubic regression on sin(x)
- remembered how to derive gradient descent from scratch
- `legend` sometimes works, sometimes doesn’t, and i’ve made peace with that
- read *Half-assing it with everything you’ve got* (https://mindingourway.com/half-assing-it-with-everything-youve-got/)

## takeaways
- deriving things on paper is surprisingly clarifying; do that more

## ideas
- build a sweep test using `logspace` to vary learning rates
- make a version of the notebook that teaches gradient descent from scratch. but the way I like it
- write a short summary of the difference between `mean()`, and `E[...]`
