## what i’m doing today

- continue with Introduction to Pytorch and Gradient Descent - Google Colab (https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/pytorch_tuto/pytorch_tuto.ipynb)
- refactor yesterday’s code into clean sections
- finally fix the `legend()` issue
- write summary notes on the Soares essay

## stream of consciousness
broke the code into tidy sections — imports, functions, data, model, loop. much easier to read, and i like seeing forward/backward/update blocks clearly.

`legend()` still didn’t work. reinstalled matplotlib. it magically started working. still don’t know what the original issue was, and maybe i don’t need to.

then came the low-sleep moment: caught myself genuinely confused why `torch.linspace(...)` and `torch.rand(...)` weren’t the same. like, they're both giving you five numbers between 0 and 1, right? nope. one is evenly spaced, the other is chaos. i froze, asked gpt, laughed out loud when i saw the answer, and then felt 80% better. still slightly curious why `rand` also “works” in practice.

read Soares more carefully today and took detailed notes. it’s a slow reading mode, but worth it.

## results
- clean modular layout for training code
- finalized the plot function (legend behaves)
- wrote notes on the Soares essay

## takeaways
- if `legend()` doesn’t work, don’t debug your soul — reinstall that lib and move on
- slowing down to read carefully (Soares) actually helped — worth doing even when tempted to skim (and yeah I disagree with authors main advise=)

## ideas
- visualize prediction stability with different input point distributions for parametrs -- why the heck is it randN()???
- connect Soares’ essay to earlier readings
