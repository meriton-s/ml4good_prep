## what i’m doing today
- finish yesterday’s notes and push them to the repo
- complete the *Introduction to PyTorch and Gradient Descent* Colab  
  (link above)  
- start the mandatory video playlist and decide on a live-note workflow
- get a first clear picture of PyTorch’s computational graph internals

## stream of consciousness
what if ml4good runs out of caffeine sources? note to self: three cans of energy drink, minimum.  
debating whether to pack the absurd fountain-pen: useless weight, but that nib practically derived half of backprop for me.

model wouldn’t train; reading code out loud fixed it (yes, variable-name mix-up again). tempted to bring a rubber duck so i can lecture it instead of the empty air.

need a deeper grasp of how the computational graph is actually stored. started digging today; will continue once the mandatory tasks are done. can’t keep relying on “short one-liners that magically work” without knowing *why*.

discovered that `torch.mean(tensor)` and `tensor.mean()` are the same op wrapped in two interfaces. syntactic sugar, but with edge cases:
```python
x = torch.tensor([1.0, 2.0, 3.0])

# convenient
x.mean()                    # ok
x.std()                     # ok

# flexible
torch.mean(x, dtype=torch.float64)        # dtype arg only here
torch.sum(x, dim=0, keepdim=True)         # keepdim only here

##technical debt reminder — broadcasting rules

only dimensions of size 1 (or missing dimensions) are stretched

comparison is right-to-left

if two sizes differ and neither is 1 → RuntimeError

first reaction to “column ** raised to ** row” was confusion; then it clicked: it’s an outer power (outer product but with ** instead of *). reshape both tensors to (n, m) virtually, then column[i] ** row[j] fills the matrix.

## results
- didn’t finish the Colab notebook (stuck at ~60 %), videos still unopened — tomorrow’s problem
- **did** crack the computational-graph internals; have a working trace script and a mental model that finally feels solid
- airline tickets to the camp purchased — logistics no longer hanging over my head

##takeaways
reading code aloud still wins over staring at the screen
