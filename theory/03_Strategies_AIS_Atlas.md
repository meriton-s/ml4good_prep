# AI Safety Atlas — Chapter 03 - Strategies
https://ai-safety-atlas.com/chapters/03/

## 3.1 ai safety is challenging
- nature of ai: black-box systems trained, not hand-coded — behaviors often surprise us  
- key point: interpretability tools lag behind, so we lack off-the-shelf safety primitives  

### quotes
> aIs are black boxes that are trained, not built…behaviors exhibited by deep neural networks are not well understood and keep surprising us.”  
- “finding solutions when you don’t know there is a problem is hard.”  

## 3.2 definitions
- alignment = goal harmony  
- safety = harm mitigation  
- ethics = moral principles  
- control = override mechanisms  
- key point: precise terms prevent conflation and confusion  

> alignment seeks to prevent preference divergence by design, while control deals with the consequences of potential divergence after the fact.

> the main concern is whether the AI is pursuing the objectives we want it to pursue.

## 3.3 preventing misuse
- layers: locked APIs for high-risk models, defense acceleration (d/acc), norms & regulation  
- key point: trade openness for security to slow down malicious adoption  

### quotes
> once a model is freely accessible, even if it has been fine-tuned to include security filters, removing these filters is relatively straightforward.

> d/acc is a strategic approach focusing on promoting technologies and innovations that inherently favor defense over offense.

### my thoughts
- how do we enforce API lockdown in jurisdictions without strong regulation?  
- could d/acc backfire by creating complacency instead of genuine caution?  


## 3.4 agi safety
- approaches: scalable oversight (proof-carrying code, human-in-loop), robust spec (reward modeling), interpretability  
- key point: solutions must scale with model capability and minimize “alignment tax”  

### quotes
> scalability: the solution should be able to scale with the AI’s intelligence. 

> alignment tax refers to the extra effort, costs, and trade-offs involved in ensuring that AIs are aligned with human values and goals.

### my thoughts
- what’s an acceptable alignment tax before labs walk away?

## 3.5 asi safety
- strands: superalignment (AI-assisted alignment research), safe-by-design architectures, global coordination  
- key point: anticipate power-seeking before superintelligence emerges  


### my thoughts
- (question) is ‘myth of inevitability’ framing a helpful counter-narrative or too idealistic? 

## 3.6 systemic safety
- focus: governance reform, org-level risk culture (Swiss-cheese model), sociotechnical fixes for bias & well-being  
- key point: tech alone won’t solve systemic harms — need policy and culture  

### quotes
> AI safety is a socio-technical problem that requires a socio-technical solution.

### my thoughts
- (question) what’s the role of civil society vs governments in governance reform?  

## 3.7 conclusion
- takeaway: no single fix — layered, cross-disciplinary strategies are essential  
- next step: coordinate efforts across research, policy, industry, and public engagement  

### my thoughts
- which stakeholder group is most under-engaged today?  
- how do we avoid ‘checklist’ culture and actually integrate layers?  